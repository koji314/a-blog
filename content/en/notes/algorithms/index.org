#+TITLE: COMP3506 Exam Preperation
#+AUTHOR: Ismael Khan
#+DATE: 2020-11-12
A dumping of general COMP3506 notes within this notebook, simply because I have done goofed my notes, these are *not* formal notes, these are just scratch notes to help with recall on
the exam.

{{<katex "display">}}
{{</katex>}}
* Algorithm Analysis
Formally we define \(\Omega\) notation if \(f(n)\) is asymptotically *greater than or equal to* \(g(n)\). a

*Definition: \(\Omega\)*: \(f(n)\) is \(\Omega(g(n))\) if there exists some \(c > 0\) and \(n_0 > 0\) such that
\[
f(n) \geq c \cdot g(n) \; \; \forall n \geq n_0
\]


Analagously, we define \(\Theta\) notation as asymptotically equal to \(g(n)\).

*Definition \(\Theta\)*: \(f(n)\) is \(\Theta(g(n))\) if there exists some \(c_1, c_2 > 0\) and \(n_0 > 0\) such that
\[
c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot f(n) \;\; \forall n \geq n_0
\]

* Linear Data Structures
*Linear Data Structures* is a data structure where the elements are represented as some form of line or sequence. More particularly, *one element follows the next*. Examples of Linear Structures are
- Lists
- Stacks
- Queues
- Vectors
- Sequences

In a programming sense, what could we implement the linear structures with? Arrays?

If we use *arrays*, we need to declare the specific size of the array on creation. We then get \(O(1)\) time to get random access. Resizing, prepending, appending and removal happen all from the head of the array (the end).
** Singly/Doubly Linked Lists
*** Singly Linked List
A singly linked list is a sequence of nodes that store both the value and *the pointer to the next node*.
[[./img/singly.png]]
*** Doubly Linked List
A doubly linked list is a sequence of nodes that store the value, *the pointer to the previous node and the pointer to the next node*.

Generally most linked lists are implemented as *doubly*, this is because being able to go back and forth between nodes in the sequence is much more convinient that in a *singly*. The disadvantage to a *doubly* is that per node, it takes up one extra node in memory (unless it is implemented like [[https://en.wikipedia.org/wiki/XOR_linked_list][this]]).

Generally for a Linked Implementation, the efficiency of the operations are as follows:
- Accessing an element: \(O(1)\)
- Iterating over elements: \(O(n)\)
- Insert / Delete element: \(O(1)\)
- Memory usage: \(O(n)\)
** Stacks
Stores arbitrary objects, insertions and deletions are LIFO (last in, first out). Analagous to a spring-loaded plate dispenser, where we insert at the top of the dispenser, and the first thing to come out is also from the top. Thus
- Insertions at the top of the stack
- Removals from the top of the stack

*** Array-based Stack
A simple stack implementation is just adding elements from left to right and keeping track on what the index of the top element of the stack. At a point the array might become full, in which a =push= operation should throw some kind of =IllegalStateException.=

**** Performance of Array-based Stack
If \(n\) is the amount of elements in the stack, then we need an array of size \(N\) such that \(N \geq n\), thus the space used is \(O(N)\) and each operation will run in \(O(1)\) time.
** Queues
Stores arbitrary objects like the stack, but the insertion and deletions are *FIFO* (first-in, first out). Like a waiting line (queue). The insertions are at the end of the queue, and the removals are at the front of the queue.
*** Array-based Queue
Use an array of size \(N\), keeping internal track of what the index of the *front element \(f\)*, and the *number of stored elements* \(n\). Thus the rear element (removal element) is given by \(r = (f+n) \mod N\).
[[./img/array-queue.png]]

=enqueue= operation adds an element to the queue and =dequeue= pops the front element out returns it. In the array-based implementation, if the array is full, =enqueue= should throw an exception.

** Applications of Stacks
A java example on *reversing an array* (from COMP3506 course slides)
#+BEGIN_SRC java
public class Tester<V> {
    // … other methods
    public void reverseArray(V a[]) {
        Stack<V> s = new ArrayStack<>(a.length);
        for(V value: a) {
            s.push(value);
        }
        int i = 0;
        while(!s.isEmpty()) {
            a[i++] = s.pop();
        }
#+END_SRC

*** Parantheses Matching
Another java example on seeing if a string expression has all matching pairs of parenethesis. For example
- =()(()){[()]}= will return *true*
- =({[]})}= will return *false*
#+BEGIN_SRC java
public static boolean isMatched(String expression) {
    final String opening = "({["; // opening delimiters
    final String closing = ")}]"; // respective closing delimiters
    Stack<Character> buffer = new LinkedStack<>( );
    for (char c : expression.toCharArray( )) {
        if (opening.indexOf(c) != −1) // this is a left delimiter
            buffer.push(c);
        else if (closing.indexOf(c) != −1) { // this is a right delimiter
            if (buffer.isEmpty( )) // nothing to match with
                return false;
            if (closing.indexOf(c) != opening.indexOf(buffer.pop( )))
                return false; // mismatched delimiter
        }
    }
    return buffer.isEmpty( ); // were all opening delimiters matched?
}
#+END_SRC
*** HTML Tag Matching
Similar to the parenethesis matching, HTML tag matching should check if there is a match for each =<name>= with its corresponding =</name>=.
#+BEGIN_SRC java
public static boolean isHTMLMatched(String html) {
    Stack<String> buffer = new LinkedStack<>( );
    int j = html.indexOf('<‘); // find first ’<’ character (if any)
    while (j != −1) {
        int k = html.indexOf('>', j+1); // find next ’>’ character
        if (k == −1)
            return false; // invalid tag
        String tag = html.substring(j+1, k); // strip away < >
        if (!tag.startsWith("/")) // this is an opening tag
            buffer.push(tag);
        else { // this is a closing tag
            if (buffer.isEmpty( ))
                return false; // no tag to match
            if (!tag.substring(1).equals(buffer.pop( ))) // skip over '/' of tag
                return false; // mismatched tag
        }
        j = html.indexOf('<', k+1); // find next ’<’ character (if any)
    }
    return buffer.isEmpty( ); // were all opening tags matched?
}
#+END_SRC

** Application of Queues
*** Round Robin Scheduler
A round robin scheduler can be implemented by repeating the following steps
1. =e = Q.dequeue()= (get the front element of the queue)
2. Do whatever you want with =e=.
3. =Q.enqueue(e)= (chuck =e= back into the Queue)
* Priority Queues and Heaps
** Priority Queues
*Queues* work as a FIFO system, what you put in first comes out first.

*Priority Queues* stores items as _entries_. The entry with the *highest priority* is removed first, in this case it is the one with the smallest key.

*** Entry and Compare ADT in Java
The entry in a priority queue is a key pair value with getter methods
- ~getKey~
- ~getValue~

A generic priority queue will use an auxilary comparator ~compare(a,b)~ that returns
- \(i < 0\) if \(a < b\)
- \(i = 0\) if \(a = b\)
- \(i > 0\) if \(a > b\)
and an error if \(a,b\) cannot be compared.

*** Sequence-Based Prioirity Queue
An implementation of a sequence based priority queue that is *not sorted*, then ~insertion~ takes \(O(1)\) time and ~removeMin~ and ~min~ takes \(O(n)\) time.

~insert~ takes \(O(1)\) because ordering doesn't matter, we can put it at the end or beginning of the sequence or anywhere without problem. ~removeMin~ and ~min~ take \(O(n)\) because we need to scroll through the sequence and compare each value to find the minimum.

An implementation that *is sorted*, will have ~insert~ take \(O(n)\) and ~removeMin~ and ~min~ take \(O(n)\), this is as we need to compare the value \(v\) to be inserted with the values of the sequence until it finds one that is greater or equal to \(v\). Analogously, ~removeMin~ and ~min~ take \(O(1)\) because we pop off the first or last element of the sequence for an ascending or descending sequence respectively.
** Heaps
Heaps are binary trees that satisfy the following properties
- _Heap-Order_: For every internal node other than the root, the key of the child must be greater or equal to the key of the parent.
  \[
  \text{key}(v) \geq \text{key}(\text{parent}(v))
  \]
- _Complete Binary Tree_: For depths \(i = 0 \to h-1\) where \(h\) is the height of the tree, the tree must be *complete*. In other words, at each depth \(i\), there must be \(2^i\) nodes. At the last depth \(h\) however, we must have all nodes pushed to the left.
- _Last Node_: We call the rightmost node at the last depth \(h\) the "last node" of the heap.


*Theorem*: A heap storing \(n\) keys has a search height of \(O(\log n)\)

*Proof*: Let \(h\) be the height of the tree storing \(n\) keys. At depth \(h\), we have \(2^h\) nodes, then \(n \geq 2^h\) which implies that \(h \leq \log_2 n\). Thus search height is \(O(log n)\).

*** Implementing Heaps with PQ's
We set each node as a key,value pair; and keep track of the position of the last node.
[[./img/heap-pq.png]]

**** Insertion for Heaps
Insertion of a key \(k\) in a heap correlates with a priority queue ADT.

*Algorithm for Insertion*:
1. Find the insertion node \(z\) (and set \(z\) as the new last node)
2. Store the key \(k\) in \(z\)
3. Restore the heap-order with upheap.

*Upheap Algorithm*: Traverse upwards from a node \(v\) and when a node \(e\) is greater or equal to \(v\), we swap until we reach a node that is less than \(v\) or we reach the root. This algorithm runs in \(O(\log n)\) time.
**** Removal for Heaps
Removal of the last node correlates directly with priority queue ~removeMin~.

*Algorithm for Removal*:
1. Swap the root key with the key of the last node.
2. Remove the last node
3. Restore heap-order with downheap.

*Downheap Algorithm*: Traverse downwards from the root node \(v\)
- If there is no right child, we choose the left
- Otherwise if there is both, we choose the one with the smallest key.
Traverse downwards and swap with child if \(\text{key}(\text{child}) < \text{key}(v)\). This algorithm runs in \(O(\log n)\) time.
* Trees
** Binary Search Trees (BST)
** AVL Trees
** Splay Trees
Utilises an operation called "splaying" that brings a node in question up to root (self balancing), all other tree operations utilise the base "splay" operation. We define splaying with the following rules. Given some node \(x\), we let \(p\) be the parent of this node \(x\), then
1. If \(p\) is the root, then we "rotate" the tree along the edge between \(p\) and \(x\), \(px\). All children on the right *stay* on the right, vice-versa on the left.
2. If \(p\) is not the root, and \(p\) and \(x\) are *both* left children or right children, then denote \(p\)'s parent \(g\), and we rotate along \(pg\), then \(px\).
3. If \(p\) is not the root, and either \(p\) is a right child and \(x\) is a left (vice versa), then we rotate between \(px\) and we rotate along \(xg\).
** (2,4) Trees
A multi-way search tree with the key property on having *at most* _four_ children. It also requires that *all external nodes have the same depth*.

Depending on the number of children, an internal node in the tree is either called a 2, 3 or 4 node (based on how many children that node has).
*** Searching through a (2,4) tree
Searching through with a height of \(h\) takes \(O(h)\) time. Considering that each node must have at most 4 children, there are at least \(2^i\) items at some depth \(i\), and then at the final height \(h\), there must be no items. Thus at \(h-1\) we have \(2^{h-1}\) items, thus \(h \leq \log_2(n+1)\). Furthermore searching will take \(O(\log_2(n)) = O(log(n))\) time.

*** Key Operations
**** Insertion
We insert a new item, \((k,o)\) at the parent \(v\) of the leaf when we are searching for \(k\). This preserves the depth property, but has the off chance of causing an overflow making a node a \(5\) node.
[[./img/5-node-overflow.png]]
# Should be visible in webpage.
# Uncomment following to see in org

# [[./5-node-overflow.png]]
We can combat the overflow with something called a split operation, simply taking the children of the node \(v\) to be \(v_1, \dots, v_5\) and the keys \(k_1, \dots k_4\) of \(v\), we *split* \(v\) into two. We take \(v'\) to be a 3-node with \(k_1\) and \(k_2\), with children \(v_1, v_2\) and \(v_3\). \(v''\) to be a 2-node with \(k_4\) and children \(v_4, v_5\), and \(k_3\) is put into the parent of \(v\) (this is so we can split in the first place without violating rules of 2-4).
**** Deletion
If an item is at node with leaf children, then we can just delete it without hurting any other items. Otherwise if an item has children, we delete it and let the in-order successor take its place.

Similarly to insertion, this can cause *underflow*. We might have a node that has one node and no keys. This is dealt with case by case:
- Case 1: If the adjacent sibling to empty node \(v\) are 2-nodes.

  Suppose the adjacent 2-node sibling is denoted \(w\), we fuse the two by taking the single key in \(w\) and a key from the parent node \(u\).
  [[./img/fusion.png]]

- Case 2: If adjacent sibling is a 3-node or a 4-node

  If this is the case, then given \(w\) as an adjacent sibling to \(v\), and \(u\) the parent to \(v\).
  + Then we move a child of \(w\) to \(v\) (\(v\) becomes a 2-node).
  + Move an item from \(u\) to \(v\)
  + Move an item from \(w\) to \(u\)

   
***** Analysis of Deletion
Given a (2,4) tree with \(n\) items, in a deletion operation, we visit \(O(\log n)\) nodes to search for the node and delete entry. In case of an underflow, we may need to have perhaps \(O(\log n)\) fusions, where each fusion or transfer takes atleast \(O(1)\) time. All together a (2,4) tree takes \(O(\log n)\) *time to delete an item.*

*** (A,B) Trees
(A,B) trees are a somewhat generalisation of \((2,4)\) trees in the sense that we define \(a,b\) as two integers such that
\[
2 \leq a \leq (b+1)/2
\]
with the following restrictions.
+ Each internal node must have alteast \(a\) children except the root. The root has *at most* \(b\) children.
+ All external nodes *have the same depth*.

***** Height of (A,B) tree
We analyse the height of the \((a,b)\) tree similarly to the \((2,4)\), storing \(n\) entries we have *at least*
\[
\Omega\left(\frac{\log n}{\log b}\right) \implies \Omega(\log_b n)
\]
and *at most*
\[
O\left(\frac{\log n}{\log a}\right) \implies O(\log_a n)
\]

**** Searching and Key Operations
Searching functions the same as any multi-way search tree.

*Insertion* functions similar to a (2,4) tree, if we have an overflow of \(b+1\) node, we do the split operation.

*Deletion* is the same, if we have an underflow of an \(a-1\) node, we either do a transfer or a fusion depending on the sibling node.

**** B-Trees
B-Trees are a special kind of \((a,b)\) tree structure, it is well known for maintaining a map of external memory.

We say a *B-tree of order \(d\)* is an \((a,b)\) tree with \(a = \frac{d}{2}\) and \(b = d\).
# Put an image of a B-tree here pls.

*** Red-Black Trees
If we compare the \((2,4)\) tree structure to the Red-Black tree structure, we notice it has the following key details:
- The *same \(O(\log n)\) worst case complexity* for each operation; search, insert and removal.
- A arguably simpler implementation
- Fewer restructurings.
  # elaborate on above perhaps?

But what are they. They are a *binary* search tree that satisfy the following:
- The root is black
- Every leaf is black
- Children of a red node are black
- ... All leaves have the same black depth?

Since this feels like a pain to represent, take this following diagram as a conversion between a \((2,4)\) tree structure and the equivalent Red-Black tree structure.
[[./img/24-rb.png]]

**** Insertion in RB Trees
Standard insertion into a binary search tree. We colour the newly inserted node *red*, unless it is the root. This thus preserves the root, external and depth properties of the tree. Denote the node just inserted as \(x\), letting the parent of \(x\) denoted \(p\). If \(p\) is black, then it preserves the internal property as well (children of a red node is black). *Otherwise* if the parent is red, then we have a double red, which violates the interal property. So we need to reorganise the tree.

***** Fixing Double Reds
Let \(s\) be the sibling of of the parent node \(y\),
- Case 1: If \(s\) is black, we perform a *tri-node restructuring*

*Tri-Node Restructuring*: Label nodes \(x,y,z\) as \(a,b,c\) in left-right order (in order travesal), replacing \(z\) with whatever node is labelled as \(b\). Then we make nodes labelled \(a\) and \(c\) children of the node labelled \(b\).

- Case 2: if \(s\) is red, then the double red corresponds to an *overflow* in a (2,4) tree, recolouring in this case is equivalent to performing a split.

*Recolouring*: Parent \(y\) of \(x\) and its sibling become black, and grandparent of \(x\) (parent of \(y\)) becomes red unless it is the root. This is somehow equivalent to performing a split on a 5-node...

***** Analysis of Insertion
A RB Tree has height \(O(\log n)\), furthermore searching for the key \(k\), it takes \(O(\log n)\) time. Once we find \(k\), inserting the new entry takes \(O(1)\) time, each recolouring takes \(O(\log n)\) time (traversing through the tree), with each recolour taking \(O(1)\). There may be at most one restructuring, which takes \(O(1)\) time.

Overall we have insertion takes \(O(\log n)\) time in a RB tree.
**** Deletion in RB Trees
Deletion in RB trees uses a binary search tree deletion algorithm. Letting \(q\) being the internal node that is being removed, \(r\) being the external node being removed and \(p\) being the sibling of \(r\).
- If \(q\) was red, nothing more to do
- If \(q\) was *black*, both children were either external, or \(q\) has one red child \(p\). If so, we recolour \(p\) to be black.
- If \(q\) was black, with two external nodes, then we have a double black. This corresponds to an underflow in (2,4).
 


***** Fixing Double Blacks
Consider a double black node \(p\) whose sibling is denoted \(y\),
- If \(y\) is black and has a red child \(x\), we perform a *restructuring* (equivalent to transfer)
- If \(y\) is black and its children are both black, we perform a *recolouring* (equivalent to fusion)
- If \(y\) is red, we perform an *adjustment*. After the adjustment is applied we see if either the former cases apply.

Time complexity of deletion in RB trees takes \(O(\log n)\) time.

* DAG's and Topological Orderings
** Directed Acrylic Graphs
An ordering of nodes in a *direct graph* such that for each node in a path from \(A\) to \(B\), node \(A\) will be before node \(B\). The ordering is not unique.
[[./img/Topological_Ordering.svg]]

It follows that *not all graphs* have a topological ordering. For example, graphs with cycles. Only a *DAG* can have a topological ordering.

*** Topological Sorting
Algorithm for getting the topological order:
1. Select a vertex \(v\) with no incoming edges
2. Append \(v\) to the result?
3. Remove \(v\) and its outgoing edges.
* Pattern Matching
A string is simply a sequence of characters, thus it follows that the *alphabet* (denoted \(\Sigma\)) is the set of _all possible characters_ for a family of strings. Examples include
- ASCII
- Unicode
- {0,1} (Binary)
- {A,C,G,T} (DNA, RNA Sequences)

Let \(S\) be a string of size \(m\), then we say a *substring* of \(S\) is some portion of \(S\) from position \(i\) to \(j\).

A prefix is a substring that starts at position 0 (beginning of string), to some position \(i\). Similarly a suffix is a substring starting from position \(i\) to the end of the string (\(m-1\)).

The problem arises, how can we find a substring \(T\) in \(S\) that matches some pattern \(P\)?
** Brute Force Pattern Matching
Literally compare the pattern \(P\) with \(T\) for each possible character shift in \(T\).
#+BEGIN_SRC ps
Alg BruteForce(T,P)
for i = 0 to n - m do:
    { test shift at position i of the pattern }
    j = 0
    while j < m AND T[i + j] = P[j] do
        j = j + 1
    if j = m then
        return i {we found a match at pos i}
    else
        break {keep going}
return -1 {no match found}
#+END_SRC
For a pattern with size \(n\) and text \(T\) with size \(m\). The runtime complexity with brute-force pattern searching is \(O(nm)\), an example of worse case would be the following \(T = aaaaaaaah\) and \(P = aaah\).
[[./img/brute.png]]

** Boyer-Moore Pattern Matching Algorithm
The Boyer-Moore pattern matching algorithm works as follows, shifting the pattern down the string \(S\), *if a mismatch occurs* at \(S[i] = \alpha\):
- If \(\alpha \in P\), then we position \(P\) such that position \(P[\beta] = \alpha\) is aligned with \(S[i] = \alpha\).
  [[./img/pcontc.png]]
- Otherwise, we shift \(P\) such that the beginning of \(P\) (\(P[0]\)) is at \(S[i+1]\).
  [[./img/pend.png]]

Observe this very nice example (from COMP3506 lecture slides as most of these screenshots are).
[[./img/b-m-example.png]]

*** Last-Occurence Function
The Boyer-Moore's algorithm preprocesses the pattern and the alphabet to generate the *last occurence function* \(L\).

Defined \(L:\Sigma \to \mathbb{N}\) such that
\[
L(c) := \begin{cases}
\text{ largest index of } i &\text{ such that } P[i] = c\\
-1 & \text{ if no index exists }
\end{cases}
\]
# Ah this is cursed.
In processing we can represent it by some array that is indexed by the characters in some numerical format I dont know what this is trying to say tbh. Furthermore LOF is computed in \(O(m+s)\) time where \(m\) is the size of the pattern \(P\) and \(s\) is the size of the alphabet \(\Sigma\), and accessed in \(O(1)\) time.

Overall, Boyer-Moore's algorithm runs in \(O(nm + s)\) time, with \(n\) being the size of \(T\), \(m\) the size of \(P\) and \(s\) the size of the alphabet. Regardless, Boyer-Moore's algorithm is significantly *faster* than brute-force.

** Knuth-Morris-Pratt Algorithm (KMP)
The KMP algorithm compares the pattern to the text from left-to-right, however the methods and steps to shifting are more /intelligent/ than brute force. Particularly, we want to now how much we can shift to avoid redundant comparisons. In particular, this is represented by *the largest prefix of \(P[0, j-1]\) that is a suffix of \(P[1,j-1]\)*. You can observe this in the following, the suffix \(ab\) is equal to the largest prefix, thus we shift \(P\) to the index of the end of the largest prefix \(ab\). This avoids the unneccesary comparison.
[[./img/comp.png]]

In particular, the KMP algorithm will preprocess the pattern finding the largest prefix of substring \(P[0,j]\) that is also a suffix of \(P[1,j]\) for all \(j\), the size of the prefix is denoted as \(F(j)\), the *Failure Function* at \(j\). Thus when a mismatch occurs, we can set \(j\) (the shift position) to be \(F(j-1)\).
*** Analysis of Algorithm
For a pattern \(P\), with size \(m\), the Failure Function is represented by an array and takes \(O(m)\) time to be computed.
#+BEGIN_SRC ps
Algorithm KMPMatch(T, P)
F = failureFunction(P)
i = 0
j = 0
while i < length(T)
        if T[i] = P[j] then
            if j = length(P) - 1 then
                return i - j { match }
            else
                i = i + 1
                j = j + 1
        else
            if j > 0 then
                j = F[j - 1]
            else
                i = i + 1
return -1 { no match }
#+END_SRC
Each iteration of the loop, we either increment \(i\) by one *or* the shift distance increments by one. This means that we can get no more than \(2n\) iterations of the while loop. So our KMP algorithm runs in \(O(m+n)\) time!

* Tries (Re"trie"val trees)
* Text Compression
We are able to compress a string \(X\) into a smaller string \(Y\) using a *prefix code* for the characters of \(X\). The main purpose of this is to save space and memory.
** Prefix Codes
Mapping each character of an alphabet to a binary code word, such that no code-word is a prefix of another.

We can use an *encoding trie* to represent the prefix-codes. Each *external* node stores a character, and the code word is given by the path from the root to the external node of that character.

We denote an edge to a left child 0, and edge to a right child 1.
[[./img/test.png]]

** Huffman's Algorithm
Given a string \(X\), we construct a prefix code that *minimises* the size of the encoding of \(X\). (Optimization of the encoding). Using an example string \(X = abracadabra\)
1. Preprocess the frequency of each character in \(X\). Let \(f(k)\) be the frequency for each char \(k \in x\). For some reason the table isn't working in enumerations you'll just have to imagine it :)
2. Create an empty priority queue, for each character in \(X\), we create a *single node* binary tree \(T\) storing \(k\), insert \(f(k)\) as the key and \(T\) as the value.
3. *While the priority queue has more than one element* (=PQ.size() > 1=), we remove two items from the PQ and store them in \((f_1, T_1)\) and \((f_2, T_2)\) respectively. Create a *new* tree \(T'\) that has \(T_1\) as the left subtree and \(T_2\) as the right subtree. Insert a new item into the priority queue as the *sum of the frequencies* as the key and the *new* tree \(T'\) as the value (=PQ.insert(f1+f2,T)=).
4. This repeats until the entire priority queue merges into a single item with all elements represented as a binary tree, thus there is only one item in the priority queue, we exit the loop.
5. Remove the last element of the priority queue and return the tree \(T'\).
In psuedocode, (I don't know how to reference but from COMP3506 Text Compression notes)
#+BEGIN_EXAMPLE Huffman's Algorithm
Algorithm Huffman(X):
Input: string X of length n
Output:optimal encoding tree for X
Compute frequency f(c) of each character c of X
PQ = new empty Priority Queue

for each character c in alphabet of X do
    T = single node binary tree storing c
    PQ.insert(f(c), T)
while PQ.size() > 1 do
    (f1 , T1) = PQ.removeMin()
    (f2 , T2) = PQ.removeMin()
    T = a new binary tree T with left subtree T1 and right subtree T2
    PQ.insert(f 1 + f 2 , T)
(f, T) = PQ.removeMin()
return T
#+END_EXAMPLE

# #+begin_export html
# <style>
# .my-table th,
# .my-table td {
#     padding: 20px;
#     text-align: center;
# }
# </style>
# #+caption: Table for frequencies of the string \(X = abracadabra\)
# #+attr_html: :class my-table
# | \(k\)    | \(a\) | \(b\) | \(c\) | \(d\) | \(r\) |
# |----------+-------+-------+-------+-------+-------|
# | \(f(k)\) |     5 |     2 |     1 |     1 |     2 |

Under the assumption that
- The size of the string \(X\) is \(n\)
- \(d\) is the number of distinct chars of \(X\)
- The Priority Queue is implemented with a heap

Then we have that Huffman's Algorithm runs in \(O(n + d\cdot \log d)\) time. This is clear to see as the first for loop iterates throug each character in \(X\), which takes \(O(n)\) time. Each removal takes \(O(log d)\) time, which happens \(d\) times, so \(O(d\cdot \log d)\), thus as the loops happen consectively after each other, we sum them together and have \(O(n + d\cdot \log d)\) as the runtime.
