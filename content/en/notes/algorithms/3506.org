#+TITLE: COMP3506 Exam Preperation
#+AUTHOR: Ismael Khan
#+DATE: 2020-11-12
A dumping of general COMP3506 notes within this notebook, simply because I have done goofed my notes, these are *not* formal notes, these are just scratch notes to help with recall on
the exam.

{{<katex "display">}}
{{</katex>}}
* Algorithm Analysis
Formally we define \(\Omega\) notation if \(f(n)\) is asymptotically *greater than or equal to* \(g(n)\).

*Definition: \(\Omega\)*: \(f(n)\) is \(\Omega(g(n))\) if there exists some \(c > 0\) and \(n_0 > 0\) such that
\[
f(n) \geq c \cdot g(n) \; \; \forall n \geq n_0
\]


Analagously, we define \(\Theta\) notation as asymptotically equal to \(g(n)\).

*Definition \(\Theta\)*: \(f(n)\) is \(\Theta(g(n))\) if there exists some \(c_1, c_2 > 0\) and \(n_0 > 0\) such that
\[
c_1 \cdot g(n) \leq f(n) \leq c_2 \cdot f(n) \;\; \forall n \geq n_0
\]

* Priority Queues and Heaps
** Priority Queues
*Queues* work as a FIFO system, what you put in first comes out first.

*Priority Queues* stores items as _entries_. The entry with the *highest priority* is removed first, in this case it is the one with the smallest key.

*** Entry and Compare ADT in Java
The entry in a priority queue is a key pair value with getter methods
- ~getKey~
- ~getValue~

A generic priority queue will use an auxilary comparator ~compare(a,b)~ that returns
- \(i < 0\) if \(a < b\)
- \(i = 0\) if \(a = b\)
- \(i > 0\) if \(a > b\)
and an error if \(a,b\) cannot be compared.

*** Sequence-Based Prioirity Queue
An implementation of a sequence based priority queue that is *not sorted*, then ~insertion~ takes \(O(1)\) time and ~removeMin~ and ~min~ takes \(O(n)\) time.

~insert~ takes \(O(1)\) because ordering doesn't matter, we can put it at the end or beginning of the sequence or anywhere without problem. ~removeMin~ and ~min~ take \(O(n)\) because we need to scroll through the sequence and compare each value to find the minimum.

An implementation that *is sorted*, will have ~insert~ take \(O(n)\) and ~removeMin~ and ~min~ take \(O(n)\), this is as we need to compare the value \(v\) to be inserted with the values of the sequence until it finds one that is greater or equal to \(v\). Analogously, ~removeMin~ and ~min~ take \(O(1)\) because we pop off the first or last element of the sequence for an ascending or descending sequence respectively.
** Heaps
Heaps are binary trees that satisfy the following properties
- _Heap-Order_: For every internal node other than the root, the key of the child must be greater or equal to the key of the parent.
  \[
  \text{key}(v) \geq \text{key}(\text{parent}(v))
  \]
- _Complete Binary Tree_: For depths \(i = 0 \to h-1\) where \(h\) is the height of the tree, the tree must be *complete*. In other words, at each depth \(i\), there must be \(2^i\) nodes. At the last depth \(h\) however, we must have all nodes pushed to the left.
- _Last Node_: We call the rightmost node at the last depth \(h\) the "last node" of the heap.

  *Theorem*: A heap storing \(n\) keys has a search height of \(O(log n)\)

  *Proof*: Let \(h\) be the height of the tree storing \(n\) keys. At depth \(h\), we have \(2^h\) nodes, then \(n \geq 2^h\) which implies that \(h \leq \log_2 n\). Thus search height is \(O(log n)\).


*** Implementing Heaps with PQ's
We set each node as a key,value pair; and keep track of the position of the last node.
[[/img/heap-pq.png]]

**** Insertion for Heaps
Insertion of a key \(k\) in a heap correlates with a priority queue ADT.

*Algorithm for Insertion*:
1. Find the insertion node \(z\) (and set \(z\) as the new last node)
2. Store the key \(k\) in \(z\)
3. Restore the heap-order with upheap.

*Upheap Algorithm*: Traverse upwards from a node \(v\) and when a node \(e\) is greater or equal to \(v\), we swap until we reach a node that is less than \(v\) or we reach the root. This algorithm runs in \(O(\log n)\) time.
**** Removal for Heaps
Removal of the last node correlates directly with priority queue ~removeMin~.

*Algorithm for Removal*:
1. Swap the root key with the key of the last node.
2. Remove the last node
3. Restore heap-order with downheap.

*Downheap Algorithm*: Traverse downwards from the root node \(v\)
- If there is no right child, we choose the left
- Otherwise if there is both, we choose the one with the smallest key.
Traverse downwards and swap with child if \(\text{key}(\text{child}) < \text{key}(v)\). This algorithm runs in \(O(\log n)\) time.
* Trees
** Binary Search Trees (BST)
** AVL Trees
** Splay Trees
Utilises an operation called "splaying" that brings a node in question up to root (self balancing), all other tree operations utilise the base "splay" operation. We define splaying with the following rules. Given some node \(x\), we let \(p\) be the parent of this node \(x\), then
1. If \(p\) is the root, then we "rotate" the tree along the edge between \(p\) and \(x\), \(px\). All children on the right *stay* on the right, vice-versa on the left.
2. If \(p\) is not the root, and \(p\) and \(x\) are *both* left children or right children, then denote \(p\)'s parent \(g\), and we rotate along \(pg\), then \(px\).
3. If \(p\) is not the root, and either \(p\) is a right child and \(x\) is a left (vice versa), then we rotate between \(px\) and we rotate along \(xg\).
** (2,4) Trees
A multi-way search tree with the key property on having *at most* _four_ children. It also requires that *all external nodes have the same depth*.

Depending on the number of children, an internal node in the tree is either called a 2, 3 or 4 node (based on how many children that node has).
*** Searching through a (2,4) tree
Searching through with a height of \(h\) takes \(O(h)\) time. Considering that each node must have at most 4 children, there are at least \(2^i\) items at some depth \(i\), and then at the final height \(h\), there must be no items. Thus at \(h-1\) we have \(2^{h-1}\) items, thus \(h \leq \log_2(n+1)\). Furthermore searching will take \(O(\log_2(n)) = O(log(n))\) time.

*** Key Operations
**** Insertion
We insert a new item, \((k,o)\) at the parent \(v\) of the leaf when we are searching for \(k\). This preserves the depth property, but has the off chance of causing an overflow making a node a \(5\) node.
[[/img/5-node-overflow.png]]
# Should be visible in webpage.
# Uncomment following to see in org

# [[./5-node-overflow.png]]
We can combat the overflow with something called a split operation, simply taking the children of the node \(v\) to be \(v_1, \dots, v_5\) and the keys \(k_1, \dots k_4\) of \(v\), we *split* \(v\) into two. We take \(v'\) to be a 3-node with \(k_1\) and \(k_2\), with children \(v_1, v_2\) and \(v_3\). \(v''\) to be a 2-node with \(k_4\) and children \(v_4, v_5\), and \(k_3\) is put into the parent of \(v\) (this is so we can split in the first place without violating rules of 2-4).
**** Deletion
If an item is at node with leaf children, then we can just delete it without hurting any other items. Otherwise if an item has children, we delete it and let the in-order successor take its place.

Similarly to insertion, this can cause *underflow*. We might have a node that has one node and no keys. This is dealt with case by case:
- Case 1: If the adjacent sibling to empty node \(v\) are 2-nodes.

  Suppose the adjacent 2-node sibling is denoted \(w\), we fuse the two by taking the single key in \(w\) and a key from the parent node \(u\).
  [[/img/fusion.png]]

- Case 2: If adjacent sibling is a 3-node or a 4-node

  If this is the case, then given \(w\) as an adjacent sibling to \(v\), and \(u\) the parent to \(v\).
  + Then we move a child of \(w\) to \(v\) (\(v\) becomes a 2-node).
  + Move an item from \(u\) to \(v\)
  + Move an item from \(w\) to \(u\)


***** Analysis of Deletion
Given a (2,4) tree with \(n\) items, in a deletion operation, we visit \(O(\log n)\) nodes to search for the node and delete entry. In case of an underflow, we may need to have perhaps \(O(\log n)\) fusions, where each fusion or transfer takes atleast \(O(1)\) time. All together a (2,4) tree takes \(O(\log n)\) *time to delete an item.*


*** (A,B) Trees
(A,B) trees are a somewhat generalisation of \((2,4)\) trees in the sense that we define \(a,b\) as two integers such that
\[
2 \leq a \leq (b+1)/2
\]
with the following restrictions.
+ Each internal node must have alteast \(a\) children except the root. The root has *at most* \(b\) children.
+ All external nodes *have the same depth*.


***** Height of (A,B) tree
We analyse the height of the \((a,b)\) tree similarly to the \((2,4)\), storing \(n\) entries we have *at least*
\[
\Omega\left(\frac{\log n}{\log b}\right) \implies \Omega(\log_b n)
\]
and *at most*
\[
O\left(\frac{\log n}{\log a}\right) \implies O(\log_a n)
\]

**** Searching and Key Operations
Searching functions the same as any multi-way search tree.

*Insertion* functions similar to a (2,4) tree, if we have an overflow of \(b+1\) node, we do the split operation.

*Deletion* is the same, if we have an underflow of an \(a-1\) node, we either do a transfer or a fusion depending on the sibling node.

**** B-Trees
B-Trees are a special kind of \((a,b)\) tree structure, it is well known for maintaining a map of external memory.

We say a *B-tree of order \(d\)* is an \((a,b)\) tree with \(a = \frac{d}{2}\) and \(b = d\).
# Put an image of a B-tree here pls.

*** Red-Black Trees
If we compare the \((2,4)\) tree structure to the Red-Black tree structure, we notice it has the following key details:
- The *same \(O(\log n)\) worst case complexity* for each operation; search, insert and removal.
- A arguably simpler implementation
- Fewer restructurings.
  # elaborate on above perhaps?

But what are they. They are a *binary* search tree that satisfy the following:
- The root is black
- Every leaf is black
- Children of a red node are black
- ... All leaves have the same black depth?

Since this feels like a pain to represent, take this following diagram as a conversion between a \((2,4)\) tree structure and the equivalent Red-Black tree structure.
[[/img/24-rb.png]]

**** Insertion in RB Trees
Standard insertion into a binary search tree. We colour the newly inserted node *red*, unless it is the root. This thus preserves the root, external and depth properties of the tree. Denote the node just inserted as \(x\), letting the parent of \(x\) denoted \(p\). If \(p\) is black, then it preserves the internal property as well (children of a red node is black). *Otherwise* if the parent is red, then we have a double red, which violates the interal property. So we need to reorganise the tree.
***** Fixing Double Reds
Let \(s\) be the sibling of of the parent node \(y\),
- Case 1: If \(s\) is black, we perform a *tri-node restructuring*

*Tri-Node Restructuring*: Label nodes \(x,y,z\) as \(a,b,c\) in left-right order (in order travesal), replacing \(z\) with whatever node is labelled as \(b\). Then we make nodes labelled \(a\) and \(c\) children of the node labelled \(b\).

- Case 2: if \(s\) is red, then the double red corresponds to an *overflow* in a (2,4) tree, recolouring in this case is equivalent to performing a split.

*Recolouring*: Parent \(y\) of \(x\) and its sibling become black, and grandparent of \(x\) (parent of \(y\)) becomes red unless it is the root. This is somehow equivalent to performing a split on a 5-node...

***** Analysis of Insertion
A RB Tree has height \(O(\log n)\), furthermore searching for the key \(k\), it takes \(O(\log n)\) time. Once we find \(k\), inserting the new entry takes \(O(1)\) time, each recolouring takes \(O(\log n)\) time (traversing through the tree), with each recolour taking \(O(1)\). There may be at most one restructuring, which takes \(O(1)\) time.

Overall we have insertion takes \(O(\log n)\) time in a RB tree.
**** Deletion in RB Trees
Deletion in RB trees uses a binary search tree deletion algorithm. Letting \(q\) being the internal node that is being removed, \(r\) being the external node being removed and \(p\) being the sibling of \(r\).
- If \(q\) was red, nothing more to do
- If \(q\) was *black*, both children were either external, or \(q\) has one red child \(p\). If so, we recolour \(p\) to be black.
- If \(q\) was black, with two external nodes, then we have a double black. This corresponds to an underflow in (2,4).

 
***** Fixing Double Blacks
Consider a double black node \(p\) whose sibling is denoted \(y\),
- If \(y\) is black and has a red child \(x\), we perform a *restructuring* (equivalent to transfer)
- If \(y\) is black and its children are both black, we perform a *recolouring* (equivalent to fusion)
- If \(y\) is red, we perform an *adjustment*. After the adjustment is applied we see if either the former cases apply.

Time complexity of deletion in RB trees takes \(O(\log n)\) time.

* Pattern Matching
A string is simply a sequence of characters, thus it follows that the *alphabet* (denoted \(\Sigma\)) is the set of _all possible characters_ for a family of strings. Examples include
- ASCII
- Unicode
- {0,1} (Binary)
- {A,C,G,T} (DNA, RNA Sequences)

Let \(S\) be a string of size \(m\), then we say a *substring* of \(S\) is some portion of \(S\) from position \(i\) to \(j\).

A prefix is a substring that starts at position 0 (beginning of string), to some position \(i\). Similarly a suffix is a substring starting from position \(i\) to the end of the string (\(m-1\)).

The problem arises, how can we find a substring \(T\) in \(S\) that matches some pattern \(P\)?
** Brute Force Pattern Matching
Literally compare the pattern \(P\) with \(T\) for each possible character shift in \(T\).
#+BEGIN_SRC ps
Alg BruteForce(T,P)
for i = 0 to n - m do:
    { test shift at position i of the pattern }
    j = 0
    while j < m AND T[i + j] = P[j] do
        j = j + 1
    if j = m then
        return i {we found a match at pos i}
    else
        break {keep going}
return -1 {no match found}
#+END_SRC
For a pattern with size \(n\) and text \(T\) with size \(m\). The runtime complexity with brute-force pattern searching is \(O(nm)\), an example of worse case would be the following \(T = aaaaaaaah\) and \(P = aaah\).
[[/img/brute.png]]

** Boyer-Moore Pattern Matching Algorithm
The Boyer-Moore pattern matching algorithm works as follows, shifting the pattern down the string \(S\), *if a mismatch occurs* at \(S[i] = \alpha\):
- If \(\alpha \in P\), then we position \(P\) such that position \(P[\beta] = \alpha\) is aligned with \(S[i] = \alpha\).
  [[/img/pcontc.png]]
- Otherwise, we shift \(P\) such that the beginning of \(P\) (\(P[0]\)) is at \(S[i+1]\).
  [[/img/pend.png]]

Observe this very nice example (from COMP3506 lecture slides as most of these screenshots are).
[[/img/b-m-example.png]]

*** Last-Occurence Function
The Boyer-Moore's algorithm preprocesses the pattern and the alphabet to generate the *last occurence function* \(L\).

Defined \(L:\Sigma \to \mathbb{N}\) such that
\[
L(c) := \begin{cases}
\text{ largest index of } i &\text{ such that } P[i] = c\\
-1 & \text{ if no index exists }
\end{cases}
\]
# Ah this is cursed.
In processing we can represent it by some array that is indexed by the characters in some numerical format I dont know what this is trying to say tbh. Furthermore LOF is computed in \(O(m+s)\) time where \(m\) is the size of the pattern \(P\) and \(s\) is the size of the alphabet \(\Sigma\), and accessed in \(O(1)\) time.

Overall, Boyer-Moore's algorithm runs in \(O(nm + s)\) time, with \(n\) being the size of \(T\), \(m\) the size of \(P\) and \(s\) the size of the alphabet. Regardless, Boyer-Moore's algorithm is significantly *faster* than brute-force.

** Knuth-Morris-Pratt Algorithm (KMP)
The KMP algorithm compares the pattern to the text from left-to-right, however the methods and steps to shifting are more /intelligent/ than brute force. Particularly, we want to now how much we can shift to avoid redundant comparisons. In particular, this is represented by *the largest prefix of \(P[0, j-1]\) that is a suffix of \(P[1,j-1]\)*. You can observe this in the following, the suffix \(ab\) is equal to the largest prefix, thus we shift \(P\) to the index of the end of the largest prefix \(ab\). This avoids the unneccesary comparison.
[[/img/comp.png]]

In particular, the KMP algorithm will preprocess the pattern finding the largest prefix of substring \(P[0,j]\) that is also a suffix of \(P[1,j]\) for all \(j\), the size of the prefix is denoted as \(F(j)\), the *Failure Function* at \(j\). Thus when a mismatch occurs, we can set \(j\) (the shift position) to be \(F(j-1)\).
*** Analysis of Algorithm
For a pattern \(P\), with size \(m\), the Failure Function is represented by an array and takes \(O(m)\) time to be computed.
#+BEGIN_SRC ps
Algorithm KMPMatch(T, P)
F = failureFunction(P)
i = 0
j = 0
while i < length(T)
        if T[i] = P[j] then
            if j = length(P) - 1 then
                return i - j { match }
            else
                i = i + 1
                j = j + 1
        else
            if j > 0 then
                j = F[j - 1]
            else
                i = i + 1
return -1 { no match }
#+END_SRC
Each iteration of the loop, we either increment \(i\) by one *or* the shift distance increments by one. This means that we can get no more than \(2n\) iterations of the while loop. So our KMP algorithm runs in \(O(m+n)\) time!

* Tries (Re"trie"val trees)
* Text Compression
